{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b782cb-bfd3-4a32-bfd4-de57c6c1c490",
   "metadata": {},
   "source": [
    "## Doc2vec vectorization\n",
    "## Created by: Aditi Das\n",
    "## From: SVNIT, Gujarat\n",
    "## Mtech Data Science - p23ds008 (2023-25)\n",
    "## Subject: NLP Project\n",
    "## Last Updated: 29/03/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467dd398-7bd4-4401-a508-dd552b866302",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dcf39af-beca-4455-af9c-07fe8e56fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094f15d-3c48-4fe0-811e-afb8f06b2bc4",
   "metadata": {},
   "source": [
    "## Downloading NLTK Resources for performing Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dcf2965-e326-481c-9bd2-1d5f6bc042af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Harsh\n",
      "[nltk_data]     Bari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Harsh\n",
      "[nltk_data]     Bari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Harsh\n",
      "[nltk_data]     Bari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec137056-f5e3-441a-9c4b-3d0c5e463365",
   "metadata": {},
   "source": [
    "## Loading the \"train.csv\" Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c12999f-a10a-4a36-ab53-75d7d806cbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be aware  dirty step to get money  #staylight ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#sarcasm for #people who don't understand #diy...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@IminworkJeremy @medsingle #DailyMail readers ...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@wilw Why do I get the feeling you like games?...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-@TeacherArthurG @rweingarten You probably jus...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets       class\n",
       "0  Be aware  dirty step to get money  #staylight ...  figurative\n",
       "1  #sarcasm for #people who don't understand #diy...  figurative\n",
       "2  @IminworkJeremy @medsingle #DailyMail readers ...  figurative\n",
       "3  @wilw Why do I get the feeling you like games?...  figurative\n",
       "4  -@TeacherArthurG @rweingarten You probably jus...  figurative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a5cc3f-bf01-4294-8e91-2e9cadcd55c3",
   "metadata": {},
   "source": [
    "## Applying Text Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7e821f1-f6b1-41ff-a345-6e39d8bea652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Harsh\n",
      "[nltk_data]     Bari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Harsh\n",
      "[nltk_data]     Bari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Harsh\n",
      "[nltk_data]     Bari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Removing punctuation and special characters\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [token.translate(table) for token in tokens]\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    \n",
    "    # Removing numbers\n",
    "    tokens = [token for token in tokens if not token.isdigit()]\n",
    "    \n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply preprocessing to the 'tweets' column\n",
    "df['tweets_preprocessed'] = df['tweets'].apply(preprocess_text)\n",
    "\n",
    "# Save preprocessed tweets and class labels to a new CSV file\n",
    "df[['tweets_preprocessed', 'class']].to_csv(\"preprocessed_train_data.csv\", index=False)\n",
    "\n",
    "# Display the preprocessed data\n",
    "print(\"Preprocessed data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a900a0-4058-40f4-860a-b82b4e0ba4c7",
   "metadata": {},
   "source": [
    "### Displaying the Preprocessed Saved Train csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f01cd5ab-cc66-4a2d-abfc-9f46f17501db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets_preprocessed</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aware dirty step get money staylight staywhite...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarcasm people nt understand diy artattack htt...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iminworkjeremy medsingle dailymail reader sens...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wilw get feeling like game sarcasm</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>teacherarthurg rweingarten probably missed tex...</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 tweets_preprocessed       class\n",
       "0  aware dirty step get money staylight staywhite...  figurative\n",
       "1  sarcasm people nt understand diy artattack htt...  figurative\n",
       "2  iminworkjeremy medsingle dailymail reader sens...  figurative\n",
       "3                 wilw get feeling like game sarcasm  figurative\n",
       "4  teacherarthurg rweingarten probably missed tex...  figurative"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved CSV file\n",
    "preprocessedTrainData_df = pd.read_csv(\"preprocessed_train_data.csv\")\n",
    "\n",
    "# Display the content of the DataFrame\n",
    "preprocessedTrainData_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d049997-3af6-47fd-a2e5-7f77cc392c17",
   "metadata": {},
   "source": [
    "## Doc2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78bb3427-1c69-41c4-8f72-5ae1f87c33d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
      "C:\\Users\\Harsh Bari\\AppData\\Local\\Temp\\ipykernel_19952\\4286379589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets_preprocessed</th>\n",
       "      <th>class</th>\n",
       "      <th>tokens</th>\n",
       "      <th>doc2vec_1</th>\n",
       "      <th>doc2vec_2</th>\n",
       "      <th>doc2vec_3</th>\n",
       "      <th>doc2vec_4</th>\n",
       "      <th>doc2vec_5</th>\n",
       "      <th>doc2vec_6</th>\n",
       "      <th>doc2vec_7</th>\n",
       "      <th>...</th>\n",
       "      <th>doc2vec_141</th>\n",
       "      <th>doc2vec_142</th>\n",
       "      <th>doc2vec_143</th>\n",
       "      <th>doc2vec_144</th>\n",
       "      <th>doc2vec_145</th>\n",
       "      <th>doc2vec_146</th>\n",
       "      <th>doc2vec_147</th>\n",
       "      <th>doc2vec_148</th>\n",
       "      <th>doc2vec_149</th>\n",
       "      <th>doc2vec_150</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aware dirty step get money staylight staywhite...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[aware, dirty, step, get, money, staylight, st...</td>\n",
       "      <td>0.094989</td>\n",
       "      <td>-0.130547</td>\n",
       "      <td>0.209554</td>\n",
       "      <td>-0.361079</td>\n",
       "      <td>-0.221886</td>\n",
       "      <td>-0.038969</td>\n",
       "      <td>0.067625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067937</td>\n",
       "      <td>0.004318</td>\n",
       "      <td>-0.190978</td>\n",
       "      <td>-0.127249</td>\n",
       "      <td>0.224521</td>\n",
       "      <td>0.286994</td>\n",
       "      <td>-0.408927</td>\n",
       "      <td>-0.023017</td>\n",
       "      <td>0.470415</td>\n",
       "      <td>-0.181644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarcasm people nt understand diy artattack htt...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[sarcasm, people, nt, understand, diy, artatta...</td>\n",
       "      <td>0.141715</td>\n",
       "      <td>-0.156211</td>\n",
       "      <td>0.124129</td>\n",
       "      <td>-0.033891</td>\n",
       "      <td>0.164898</td>\n",
       "      <td>-0.043738</td>\n",
       "      <td>-0.018385</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092888</td>\n",
       "      <td>0.069860</td>\n",
       "      <td>-0.055245</td>\n",
       "      <td>0.069266</td>\n",
       "      <td>0.308307</td>\n",
       "      <td>0.123551</td>\n",
       "      <td>-0.235466</td>\n",
       "      <td>-0.115437</td>\n",
       "      <td>0.039865</td>\n",
       "      <td>-0.139383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iminworkjeremy medsingle dailymail reader sens...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[iminworkjeremy, medsingle, dailymail, reader,...</td>\n",
       "      <td>0.071912</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.029908</td>\n",
       "      <td>-0.157918</td>\n",
       "      <td>-0.128679</td>\n",
       "      <td>-0.045619</td>\n",
       "      <td>-0.104180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151252</td>\n",
       "      <td>-0.101986</td>\n",
       "      <td>0.282642</td>\n",
       "      <td>-0.086598</td>\n",
       "      <td>0.223873</td>\n",
       "      <td>0.174067</td>\n",
       "      <td>-0.169456</td>\n",
       "      <td>-0.119560</td>\n",
       "      <td>0.173128</td>\n",
       "      <td>-0.148945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wilw get feeling like game sarcasm</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[wilw, get, feeling, like, game, sarcasm]</td>\n",
       "      <td>-0.180418</td>\n",
       "      <td>0.049932</td>\n",
       "      <td>-0.014645</td>\n",
       "      <td>0.068700</td>\n",
       "      <td>0.355904</td>\n",
       "      <td>-0.015319</td>\n",
       "      <td>-0.081326</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058158</td>\n",
       "      <td>0.159563</td>\n",
       "      <td>-0.068043</td>\n",
       "      <td>0.086269</td>\n",
       "      <td>-0.134709</td>\n",
       "      <td>0.099647</td>\n",
       "      <td>0.085780</td>\n",
       "      <td>-0.116880</td>\n",
       "      <td>-0.046430</td>\n",
       "      <td>0.040912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>teacherarthurg rweingarten probably missed tex...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[teacherarthurg, rweingarten, probably, missed...</td>\n",
       "      <td>0.315940</td>\n",
       "      <td>-0.020790</td>\n",
       "      <td>-0.059559</td>\n",
       "      <td>-0.123925</td>\n",
       "      <td>-0.079683</td>\n",
       "      <td>-0.049342</td>\n",
       "      <td>-0.410057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079114</td>\n",
       "      <td>0.269202</td>\n",
       "      <td>-0.128115</td>\n",
       "      <td>-0.110087</td>\n",
       "      <td>0.058393</td>\n",
       "      <td>-0.061392</td>\n",
       "      <td>-0.255230</td>\n",
       "      <td>0.062121</td>\n",
       "      <td>0.095567</td>\n",
       "      <td>0.071325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 tweets_preprocessed       class  \\\n",
       "0  aware dirty step get money staylight staywhite...  figurative   \n",
       "1  sarcasm people nt understand diy artattack htt...  figurative   \n",
       "2  iminworkjeremy medsingle dailymail reader sens...  figurative   \n",
       "3                 wilw get feeling like game sarcasm  figurative   \n",
       "4  teacherarthurg rweingarten probably missed tex...  figurative   \n",
       "\n",
       "                                              tokens  doc2vec_1  doc2vec_2  \\\n",
       "0  [aware, dirty, step, get, money, staylight, st...   0.094989  -0.130547   \n",
       "1  [sarcasm, people, nt, understand, diy, artatta...   0.141715  -0.156211   \n",
       "2  [iminworkjeremy, medsingle, dailymail, reader,...   0.071912  -0.202637   \n",
       "3          [wilw, get, feeling, like, game, sarcasm]  -0.180418   0.049932   \n",
       "4  [teacherarthurg, rweingarten, probably, missed...   0.315940  -0.020790   \n",
       "\n",
       "   doc2vec_3  doc2vec_4  doc2vec_5  doc2vec_6  doc2vec_7  ...  doc2vec_141  \\\n",
       "0   0.209554  -0.361079  -0.221886  -0.038969   0.067625  ...     0.067937   \n",
       "1   0.124129  -0.033891   0.164898  -0.043738  -0.018385  ...    -0.092888   \n",
       "2   0.029908  -0.157918  -0.128679  -0.045619  -0.104180  ...     0.151252   \n",
       "3  -0.014645   0.068700   0.355904  -0.015319  -0.081326  ...    -0.058158   \n",
       "4  -0.059559  -0.123925  -0.079683  -0.049342  -0.410057  ...     0.079114   \n",
       "\n",
       "   doc2vec_142  doc2vec_143  doc2vec_144  doc2vec_145  doc2vec_146  \\\n",
       "0     0.004318    -0.190978    -0.127249     0.224521     0.286994   \n",
       "1     0.069860    -0.055245     0.069266     0.308307     0.123551   \n",
       "2    -0.101986     0.282642    -0.086598     0.223873     0.174067   \n",
       "3     0.159563    -0.068043     0.086269    -0.134709     0.099647   \n",
       "4     0.269202    -0.128115    -0.110087     0.058393    -0.061392   \n",
       "\n",
       "   doc2vec_147  doc2vec_148  doc2vec_149  doc2vec_150  \n",
       "0    -0.408927    -0.023017     0.470415    -0.181644  \n",
       "1    -0.235466    -0.115437     0.039865    -0.139383  \n",
       "2    -0.169456    -0.119560     0.173128    -0.148945  \n",
       "3     0.085780    -0.116880    -0.046430     0.040912  \n",
       "4    -0.255230     0.062121     0.095567     0.071325  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the preprocessed data from CSV\n",
    "df = pd.read_csv(\"preprocessed_train_data.csv\")\n",
    "\n",
    "# Define a function to tokenize the preprocessed text\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Tokenize the preprocessed text\n",
    "df['tokens'] = df['tweets_preprocessed'].apply(tokenize_text)\n",
    "\n",
    "# Create TaggedDocuments from tokenized text\n",
    "tagged_data = [TaggedDocument(words=doc, tags=[i]) for i, doc in enumerate(df['tokens'])]\n",
    "\n",
    "# Train Doc2Vec model\n",
    "vector_size = 150\n",
    "model = Doc2Vec(vector_size=vector_size, min_count=1, epochs=40)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Extract Doc2Vec embeddings\n",
    "doc2vec_embeddings = [model.infer_vector(doc) for doc in df['tokens']]\n",
    "\n",
    "# Add Doc2Vec embeddings to DataFrame\n",
    "for i in range(vector_size):\n",
    "    df[f\"doc2vec_{i+1}\"] = [embedding[i] for embedding in doc2vec_embeddings]\n",
    "\n",
    "# Display the DataFrame with Doc2Vec embeddings\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e260ba3c-bd30-4b45-93b8-d31a5dfd982f",
   "metadata": {},
   "source": [
    "## Saving the Resultant Doc2Vec Embedded Text into a csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c69aa80c-d71b-4f4a-958b-8777916d222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"resultant_train_dataframe_with_doc2vec.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bfd933-a670-4233-9756-157218e18d27",
   "metadata": {},
   "source": [
    "### Displaying the Saved csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8475ccf1-fe73-4e3e-b363-0207ec8e53d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets_preprocessed</th>\n",
       "      <th>class</th>\n",
       "      <th>tokens</th>\n",
       "      <th>doc2vec_1</th>\n",
       "      <th>doc2vec_2</th>\n",
       "      <th>doc2vec_3</th>\n",
       "      <th>doc2vec_4</th>\n",
       "      <th>doc2vec_5</th>\n",
       "      <th>doc2vec_6</th>\n",
       "      <th>doc2vec_7</th>\n",
       "      <th>...</th>\n",
       "      <th>doc2vec_141</th>\n",
       "      <th>doc2vec_142</th>\n",
       "      <th>doc2vec_143</th>\n",
       "      <th>doc2vec_144</th>\n",
       "      <th>doc2vec_145</th>\n",
       "      <th>doc2vec_146</th>\n",
       "      <th>doc2vec_147</th>\n",
       "      <th>doc2vec_148</th>\n",
       "      <th>doc2vec_149</th>\n",
       "      <th>doc2vec_150</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aware dirty step get money staylight staywhite...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>['aware', 'dirty', 'step', 'get', 'money', 'st...</td>\n",
       "      <td>0.094989</td>\n",
       "      <td>-0.130547</td>\n",
       "      <td>0.209554</td>\n",
       "      <td>-0.361079</td>\n",
       "      <td>-0.221886</td>\n",
       "      <td>-0.038969</td>\n",
       "      <td>0.067625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067937</td>\n",
       "      <td>0.004318</td>\n",
       "      <td>-0.190978</td>\n",
       "      <td>-0.127249</td>\n",
       "      <td>0.224521</td>\n",
       "      <td>0.286994</td>\n",
       "      <td>-0.408927</td>\n",
       "      <td>-0.023017</td>\n",
       "      <td>0.470415</td>\n",
       "      <td>-0.181644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarcasm people nt understand diy artattack htt...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>['sarcasm', 'people', 'nt', 'understand', 'diy...</td>\n",
       "      <td>0.141715</td>\n",
       "      <td>-0.156211</td>\n",
       "      <td>0.124129</td>\n",
       "      <td>-0.033891</td>\n",
       "      <td>0.164898</td>\n",
       "      <td>-0.043738</td>\n",
       "      <td>-0.018385</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092888</td>\n",
       "      <td>0.069860</td>\n",
       "      <td>-0.055245</td>\n",
       "      <td>0.069266</td>\n",
       "      <td>0.308308</td>\n",
       "      <td>0.123551</td>\n",
       "      <td>-0.235466</td>\n",
       "      <td>-0.115437</td>\n",
       "      <td>0.039865</td>\n",
       "      <td>-0.139383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iminworkjeremy medsingle dailymail reader sens...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>['iminworkjeremy', 'medsingle', 'dailymail', '...</td>\n",
       "      <td>0.071912</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.029908</td>\n",
       "      <td>-0.157918</td>\n",
       "      <td>-0.128679</td>\n",
       "      <td>-0.045619</td>\n",
       "      <td>-0.104180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151252</td>\n",
       "      <td>-0.101986</td>\n",
       "      <td>0.282642</td>\n",
       "      <td>-0.086598</td>\n",
       "      <td>0.223873</td>\n",
       "      <td>0.174067</td>\n",
       "      <td>-0.169456</td>\n",
       "      <td>-0.119560</td>\n",
       "      <td>0.173128</td>\n",
       "      <td>-0.148945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wilw get feeling like game sarcasm</td>\n",
       "      <td>figurative</td>\n",
       "      <td>['wilw', 'get', 'feeling', 'like', 'game', 'sa...</td>\n",
       "      <td>-0.180418</td>\n",
       "      <td>0.049932</td>\n",
       "      <td>-0.014645</td>\n",
       "      <td>0.068700</td>\n",
       "      <td>0.355904</td>\n",
       "      <td>-0.015319</td>\n",
       "      <td>-0.081326</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058158</td>\n",
       "      <td>0.159563</td>\n",
       "      <td>-0.068043</td>\n",
       "      <td>0.086269</td>\n",
       "      <td>-0.134709</td>\n",
       "      <td>0.099647</td>\n",
       "      <td>0.085780</td>\n",
       "      <td>-0.116880</td>\n",
       "      <td>-0.046430</td>\n",
       "      <td>0.040912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>teacherarthurg rweingarten probably missed tex...</td>\n",
       "      <td>figurative</td>\n",
       "      <td>['teacherarthurg', 'rweingarten', 'probably', ...</td>\n",
       "      <td>0.315940</td>\n",
       "      <td>-0.020790</td>\n",
       "      <td>-0.059559</td>\n",
       "      <td>-0.123925</td>\n",
       "      <td>-0.079683</td>\n",
       "      <td>-0.049342</td>\n",
       "      <td>-0.410057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079114</td>\n",
       "      <td>0.269202</td>\n",
       "      <td>-0.128115</td>\n",
       "      <td>-0.110087</td>\n",
       "      <td>0.058393</td>\n",
       "      <td>-0.061392</td>\n",
       "      <td>-0.255230</td>\n",
       "      <td>0.062121</td>\n",
       "      <td>0.095567</td>\n",
       "      <td>0.071325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 tweets_preprocessed       class  \\\n",
       "0  aware dirty step get money staylight staywhite...  figurative   \n",
       "1  sarcasm people nt understand diy artattack htt...  figurative   \n",
       "2  iminworkjeremy medsingle dailymail reader sens...  figurative   \n",
       "3                 wilw get feeling like game sarcasm  figurative   \n",
       "4  teacherarthurg rweingarten probably missed tex...  figurative   \n",
       "\n",
       "                                              tokens  doc2vec_1  doc2vec_2  \\\n",
       "0  ['aware', 'dirty', 'step', 'get', 'money', 'st...   0.094989  -0.130547   \n",
       "1  ['sarcasm', 'people', 'nt', 'understand', 'diy...   0.141715  -0.156211   \n",
       "2  ['iminworkjeremy', 'medsingle', 'dailymail', '...   0.071912  -0.202637   \n",
       "3  ['wilw', 'get', 'feeling', 'like', 'game', 'sa...  -0.180418   0.049932   \n",
       "4  ['teacherarthurg', 'rweingarten', 'probably', ...   0.315940  -0.020790   \n",
       "\n",
       "   doc2vec_3  doc2vec_4  doc2vec_5  doc2vec_6  doc2vec_7  ...  doc2vec_141  \\\n",
       "0   0.209554  -0.361079  -0.221886  -0.038969   0.067625  ...     0.067937   \n",
       "1   0.124129  -0.033891   0.164898  -0.043738  -0.018385  ...    -0.092888   \n",
       "2   0.029908  -0.157918  -0.128679  -0.045619  -0.104180  ...     0.151252   \n",
       "3  -0.014645   0.068700   0.355904  -0.015319  -0.081326  ...    -0.058158   \n",
       "4  -0.059559  -0.123925  -0.079683  -0.049342  -0.410057  ...     0.079114   \n",
       "\n",
       "   doc2vec_142  doc2vec_143  doc2vec_144  doc2vec_145  doc2vec_146  \\\n",
       "0     0.004318    -0.190978    -0.127249     0.224521     0.286994   \n",
       "1     0.069860    -0.055245     0.069266     0.308308     0.123551   \n",
       "2    -0.101986     0.282642    -0.086598     0.223873     0.174067   \n",
       "3     0.159563    -0.068043     0.086269    -0.134709     0.099647   \n",
       "4     0.269202    -0.128115    -0.110087     0.058393    -0.061392   \n",
       "\n",
       "   doc2vec_147  doc2vec_148  doc2vec_149  doc2vec_150  \n",
       "0    -0.408927    -0.023017     0.470415    -0.181644  \n",
       "1    -0.235466    -0.115437     0.039865    -0.139383  \n",
       "2    -0.169456    -0.119560     0.173128    -0.148945  \n",
       "3     0.085780    -0.116880    -0.046430     0.040912  \n",
       "4    -0.255230     0.062121     0.095567     0.071325  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved CSV file\n",
    "Doc2Vec_df = pd.read_csv(\"resultant_train_dataframe_with_doc2vec.csv\")\n",
    "\n",
    "# Display the content of the DataFrame\n",
    "Doc2Vec_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
