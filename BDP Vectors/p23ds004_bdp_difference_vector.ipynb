{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b15ec8",
   "metadata": {},
   "source": [
    "## BDP Difference Vector\n",
    "## Created by: Harsh Bari\n",
    "## From: SVNIT, Gujarat\n",
    "## Mtech Data Science - p23ds004 (2023-25)\n",
    "## Subject: NLP Project\n",
    "## Last Updated: 29/03/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0fbc566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce49305",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e96b0c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tweets       class  target\n",
      "0      Be aware  dirty step to get money  #staylight ...  figurative     0.0\n",
      "1      #sarcasm for #people who don't understand #diy...  figurative     0.0\n",
      "2      @IminworkJeremy @medsingle #DailyMail readers ...  figurative     0.0\n",
      "3      @wilw Why do I get the feeling you like games?...  figurative     0.0\n",
      "4      -@TeacherArthurG @rweingarten You probably jus...  figurative     0.0\n",
      "...                                                  ...         ...     ...\n",
      "81403  Photo: Image via We Heart It http://t.co/ky8Nf...     sarcasm     1.0\n",
      "81404  I never knew..I better put this out to the Uni...     sarcasm     1.0\n",
      "81405  hey just wanted to say thanks @ puberty for le...     sarcasm     1.0\n",
      "81406  I'm sure coverage like the Fox News Special â€œT...     sarcasm     1.0\n",
      "81407  @skeyno16 at u13?! I won't believe it until I ...     sarcasm     1.0\n",
      "\n",
      "[81408 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "461f1d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "figurative    21238\n",
       "irony         20894\n",
       "sarcasm       20681\n",
       "regular       18595\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d0a655",
   "metadata": {},
   "source": [
    "### BDP (Base Difference Protocol) Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91a023ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bdp_difference_vector as bdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aba20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bdp_train_vectors = bdp.get_vectorized(data['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b53e7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.0%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.1%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r",
      "[#=================================================] 0.2%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##################################################] 100.0%\r"
     ]
    }
   ],
   "source": [
    "input_vec = bdp.get_vectorized(data['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "097361c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import normalize\n",
    "\n",
    "# input_vec = normalize(bdp_train_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff6fb88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(array_2d, ranges_to_copy):\n",
    "    copied_ranges = []\n",
    "\n",
    "    # Loop through each range and copy the corresponding elements\n",
    "    for start, end in ranges_to_copy:\n",
    "        copied_range = array_2d[start:end+1]  # Adjust end index to include the last element\n",
    "        copied_ranges.append(copied_range)\n",
    "\n",
    "    # Concatenate the copied ranges along the first axis to create the final array\n",
    "    copied_array = np.concatenate(copied_ranges, axis=0)\n",
    "\n",
    "    return copied_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9107bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70%\n",
    "# x_train = split_data(input_vec, [(0, 14865), (21238, 35862), (42132, 55147), (60727, 75202)])\n",
    "# x_test = split_data(input_vec, [(14866, 21237), (35863, 42131), (55148, 60726), (75203, 81407)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c24b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80%\n",
    "x_train = split_data(input_vec, [(0, 16989), (21238, 37952), (42132, 57007), (60727, 77270)])\n",
    "x_test = split_data(input_vec, [(16990, 21237), (37953, 42131), (57008, 60726), (77271, 81407)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7321cfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train: 65125\n",
      "x test: 16283\n",
      "Total: 81408\n"
     ]
    }
   ],
   "source": [
    "print(\"x train:\", len(x_train))\n",
    "print(\"x test:\", len(x_test))\n",
    "print(\"Total:\", len(x_train) + len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9326d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70%\n",
    "# y_train = np.concatenate((np.zeros(14866), np.ones(27641), np.zeros(14476)))\n",
    "# y_test = np.concatenate((np.zeros(6372), np.ones(11848), np.zeros(6205)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27bd971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80%\n",
    "y_train = np.concatenate((np.zeros(16990), np.ones(31591), np.zeros(16544)))\n",
    "y_test = np.concatenate((np.zeros(4248), np.ones(7898), np.zeros(4137)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ea894f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 65125\n",
      "test: 16283\n",
      "total: 81408\n"
     ]
    }
   ],
   "source": [
    "print(\"train:\", len(y_train))\n",
    "print(\"test:\", len(y_test))\n",
    "print(\"total:\", len(y_train) + len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03a9cf",
   "metadata": {},
   "source": [
    "## BDP Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa91dcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Harsh Bari\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763f8e18",
   "metadata": {},
   "source": [
    "### Create BDP Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54a87de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdp = keras.Sequential([\n",
    "    keras.layers.Dense(256, input_shape = (150, ), activation = 'relu'),\n",
    "    keras.layers.Dense(128, activation = 'relu'),\n",
    "    keras.layers.Dense(64, activation = 'relu'),\n",
    "    keras.layers.Dense(32, activation = 'relu'),\n",
    "    keras.layers.Dense(16, activation=keras.layers.LeakyReLU(alpha=0.1)),\n",
    "    keras.layers.Dense(8, activation=keras.layers.LeakyReLU(alpha=0.1)),\n",
    "    keras.layers.Dense(2, activation = 'sigmoid')\n",
    "\n",
    "])\n",
    "\n",
    "bdp.compile(optimizer = 'adam',\n",
    "                      loss = 'sparse_categorical_crossentropy',\n",
    "                      metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9826c0",
   "metadata": {},
   "source": [
    "keras.layers.Dense(90, activation = 'relu'),\n",
    "    keras.layers.Dense(80, activation=keras.layers.LeakyReLU(alpha=0.1)),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64ea3d",
   "metadata": {},
   "source": [
    "### Check Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "037641af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 256)               38656     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82570 (322.54 KB)\n",
      "Trainable params: 82570 (322.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bdp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7bce6",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdac6f95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/22\n",
      "2036/2036 [==============================] - 9s 3ms/step - loss: 0.3966 - accuracy: 0.8022\n",
      "Epoch 2/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.3518 - accuracy: 0.8189\n",
      "Epoch 3/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.3410 - accuracy: 0.8219\n",
      "Epoch 4/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.3351 - accuracy: 0.8244\n",
      "Epoch 5/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.3307 - accuracy: 0.8263\n",
      "Epoch 6/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.3260 - accuracy: 0.8286\n",
      "Epoch 7/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.3220 - accuracy: 0.8304\n",
      "Epoch 8/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.3188 - accuracy: 0.8318\n",
      "Epoch 9/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.3154 - accuracy: 0.8329\n",
      "Epoch 10/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.3119 - accuracy: 0.8349\n",
      "Epoch 11/22\n",
      "2036/2036 [==============================] - 6s 3ms/step - loss: 0.3088 - accuracy: 0.8367\n",
      "Epoch 12/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.3055 - accuracy: 0.8386\n",
      "Epoch 13/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.3039 - accuracy: 0.8392\n",
      "Epoch 14/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.3011 - accuracy: 0.8411\n",
      "Epoch 15/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.2983 - accuracy: 0.8424\n",
      "Epoch 16/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.2952 - accuracy: 0.8435\n",
      "Epoch 17/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.2930 - accuracy: 0.8445\n",
      "Epoch 18/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.2901 - accuracy: 0.8465\n",
      "Epoch 19/22\n",
      "2036/2036 [==============================] - 6s 3ms/step - loss: 0.2888 - accuracy: 0.8467\n",
      "Epoch 20/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.2864 - accuracy: 0.8487\n",
      "Epoch 21/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.2848 - accuracy: 0.8495\n",
      "Epoch 22/22\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.2824 - accuracy: 0.8507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x14e6af89a20>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdp.fit(x_train.astype(np.float32), y_train.astype(np.float32), epochs=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56a129",
   "metadata": {},
   "source": [
    "### Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dbdeed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2036/2036 [==============================] - 5s 2ms/step - loss: 0.2756 - accuracy: 0.8536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27556082606315613, 0.853635311126709]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdp.evaluate(x_train.astype(np.float32), y_train.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b01247",
   "metadata": {},
   "source": [
    "### Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0af8e987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509/509 [==============================] - 2s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = bdp.predict(x_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f673d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(prediction, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2302c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9ab9b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.76      0.84      8385\n",
      "         1.0       0.79      0.97      0.87      7898\n",
      "\n",
      "    accuracy                           0.86     16283\n",
      "   macro avg       0.87      0.86      0.86     16283\n",
      "weighted avg       0.88      0.86      0.86     16283\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[6334 2051]\n",
      " [ 275 7623]]\n",
      "\n",
      "Accuracy: \n",
      " 0.8571516305349137\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.astype(np.float32), prediction))\n",
    "print()\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test.astype(np.float32), prediction))\n",
    "print(\"\\nAccuracy: \\n\", accuracy_score(y_test.astype(np.float32), prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
